Communicate worker list in scheduling task
 - (pin a job on workers or check that shuffles are ok after workers join)

Detect missing data on shuffle read (building on #1) and fail task
 - but with rescheduling the dependency

reschedule failed tasks on other hosts
 - heartbeat at task level to identify failed tasks

don't propagate CTRL+C to workers in ipython shell

dset.sort().first() is waaaay slower than dset.sort().count()

sort should be lazy


check on cancelling jobs
 - use https://docs.python.org/2/reference/expressions.html#generator.close ?

check on partial iteration with e.g. itake and friends

check stable ids for nodes
 - it causes a new node on an old address to be rejected
 - perhaps drop node id and use hostname and port?
   then use each address as key in peertable (node.peers)
   might just as well have issues with multiple hostnames resolving to the same machine ... 

the rx tx rates aren't updated on sp-dev
only start gui on driver by default

investigate cassandra retry policy

pickle ops in the map / filter partitions
 - this may cloudpickle functions so that this 'requirement' doesn't ripple out into the entire dset dag
same for filenames, nparrays, etc.


Limit resource usage from supervisor with: https://docs.python.org/3.4/library/resource.html
Enhance dashboard with info from https://pypi.python.org/pypi/psutil


Show cancellation of tasks / stages / jobs

Don't skip job ids for branches
(assign them on execute?)


wait for gossip to settle before starting a node / the shell
 - e.g. with ctx.worker_count
   and/or sum(t.result() for t in [w.run_task(lambda w: sum(1 for w in w.peers.filter())) for w in ctx.workers])
 - or perhaps show this as measure in the dashboard?   


move caches, broadcast values etc down to execute layer
use hierarchical namespace for set / del


Use custom pickler to set node in Execute/ComputeContext
 - allow ...Context to be more easily used in e.g. a mapper
 - ex. ctx.range(20).map(lambda i: ctx.node...)
 
 
Possibly introduce a control and data layer
 - i.e. to allow sending large volumes out of band from the control comms
 - this allows the watchdog to operate better as well
 - should improve stability
 
 

Consider not using 1 fixed worker per core, but 1 worker per node which forks per task
 - implement in execute layer
 - Use custom pickler to fetch broadcast value on receive (to ensure the data is available before forking)
 - a lazy alternative to the above would still require the broadcasted data to be available per executing process
 - caching could work through caching the data in pickled/marshalled form
   - although not having to unpickle when using cache was one of the benefits of not using pyspark
 - an advantage would be automatic cleanup after jobs have ran (less leakage)
 ! a major hurdle would be communication with e.g. Cassandra,
   - can't be inherited by a forking process and still work :(
   - would require performing the IO in the main process, and processing in a forked worker
 ! from the python docs:
   - Note that safely forking a multithreaded process is problematic.
 IDEA DROPPED:
   - the two points above are prohibitive
   - figure out an alternative memory efficient broadcast   
 
 
Consider a communication architecture with a core cross node network and a on node inter process network
 - perhaps supplemented by (temporary) direct connections on a data layer
 
 
Make entire compute.dataset api asynchronous
 - i.e. some_action() returns a future
 - use some_action().result() to get the results (may be an iterable)
 - use some_action().cancel() to cancel the job
 

Allow stages to be composed of an undetermined amount of tasks
 - i.e. allow a generator / queue of tasks
 
Allow jobs to be composed of an undetermined amount of stages
 - or put differently, give jobs the ability to yield barriers
 
The API / data model might change into a job which yields 1+ tasks and 0+ barriers.

Consider changing the task execution model from push to pull. 
 - could easy implementing having a task in flight
 - might be difficult with a generator / queue of tasks in combination with worker preferences

 
Spill shuffles to disk
 - and do the sorting in place
 
 
Implement checkpointing
 - perform cleanup of stages before the stage of the checkpointed dset 
 
 
add pickle options to broadcast_pickle
 
 
decouple yielding results from job/stage/task executions from driving their execution
  - yield returns when the receiver is done ... all that time, no tasks are scheduled
  - may check the hypotheses above ...
 
 
Prevent occurences of:
 27158 : OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 4127376 max
27158 : OpenBLAS blas_thread_init: pthread_create: Resource temporarily unavailable
 
 
make ctx.files scan dir faster
make ctx.files read files faster

 
ctx.shuffle isn't cached ?
 
 
 
CTRL+C in bndl-shell causes hangs
 
 
 
Sometimes a job/stage doesn't complete without errors ...
- Is there a race condition somewhere in job/stage execution?

PeerTable.filter is not threadsafe!

canceling a job with two+ stages only cancels the current stage? 



support cassandra.coscan with _asdicts

try to implement read retry in cassandra scan without materializing an entire token range

implement collect_as_json


implement drop function to drop columns / fields


implement icollect_unordered
use icollect_unordered for collect_as_pickles / collect_as_json



check partition sizes for cassandra scan


cassandra scan .parts() is not stable


show cancelled state of job, stage and task
show amount cancelled in status bar for job and stage


check dependency graph for dependencies going packages down/up in wrong places
 - e.g. bndl.compute.schedule to .base
move caching aspect of datasets to one module (not spread over .schedule and .base)

consider driving the tasks from another process / do something about liveness of the driver during jobs


check for a possibility to cancel jobs


make run_task async (return a handle)


support caching also for cancelled jobs / stages / tasks

nodes are 'in error' very quickly after starting a job
(irresponsive due to task deserialization?)



allow for easier debugging by reporting the failed partition (or more)


don't animate progress bar in dash, costs cpu cycles


Sort cassandra scan parts from larger to smaller
 - what about performance of .first()

 
add psize for ctx.range


add max pcount / psize for ctx.range / ctx.connection

why stage.tasks.sort(key=lambda t: t.id) ? costs time for a large numer of tasksc


stable_time = max(b - a for a, b in zip(connected_on, connected_on[1:])) * 2
 ... is killing when nodes join late!
 
 
add protocol version in hello


Support ctx.files for tar files
Support for ctx.files(split=str) for gzipped files
	- gzip has seek, but no rfind



Crash task on driver side exceptions (e.g. in sending files which are read protected)


ctx.files.cleanup creates its own garbage in node.hosted_values


broadcast actual files without loading in the driver


cache_loc not filled when using first() ???


Whooops:

	In [86]: texts.uncache()
	ERROR:bndl.rmi.node:unable to perform remote invocation
	Traceback (most recent call last):
	  File "/home/frens.jan.rumph/venv/lib/python3.4/site-packages/bndl/rmi/node.py", line 51, in _request
	    response = (yield from asyncio.wait_for(response_future, self._timeout, loop=self.peer.loop))
	  File "/usr/lib64/python3.4/asyncio/tasks.py", line 381, in wait_for
	    raise futures.TimeoutError()
	concurrent.futures._base.TimeoutError
	ERROR:bndl.rmi.node:unable to perform remote invocation
	Traceback (most recent call last):
	  File "/home/frens.jan.rumph/venv/lib/python3.4/site-packages/bndl/rmi/node.py", line 51, in _request
	    response = (yield from asyncio.wait_for(response_future, self._timeout, loop=self.peer.loop))
	  File "/usr/lib64/python3.4/asyncio/tasks.py", line 381, in wait_for
	    raise futures.TimeoutError()
	concurrent.futures._base.TimeoutError
	ERROR:bndl.rmi.node:unable to perform remote invocation
	Traceback (most recent call last):
	  File "/home/frens.jan.rumph/venv/lib/python3.4/site-packages/bndl/rmi/node.py", line 51, in _request
	    response = (yield from asyncio.wait_for(response_future, self._timeout, loop=self.peer.loop))
	  File "/usr/lib64/python3.4/asyncio/tasks.py", line 381, in wait_for
	    raise futures.TimeoutError()
	concurrent.futures._base.TimeoutError
	WARNING:bndl.rmi.node:Response <Response exception=None, req_id=48, value=None> received for unknown request id 48
	WARNING:bndl.rmi.node:Response <Response exception=None, req_id=48, value=None> received for unknown request id 48
	WARNING:bndl.rmi.node:Response <Response exception=None, req_id=45, value=None> received for unknown request id 45




and another while nodes were reconnecting to driver:
	
	KeyError: 'nl.tgho.priv.sp-prod-adg02.worker.32226.0.4'
	ERROR:asyncio:Task exception was never retrieved
	future: <Task finished coro=<_serve() done, defined at /home/frens.jan.rumph/venv/lib/python3.4/site-packages/bndl/net/peer.py:262> exception=KeyError('nl.tgho.priv.sp-prod-adg02.worker.32226.0.4',)>
	Traceback (most recent call last):
	  File "/usr/lib64/python3.4/asyncio/tasks.py", line 236, in _step
	    result = coro.send(value)
	  File "/home/frens.jan.rumph/venv/lib/python3.4/site-packages/bndl/net/peer.py", line 270, in _serve
	    yield from self.local._peer_connected(self)
	  File "/home/frens.jan.rumph/venv/lib/python3.4/site-packages/bndl/net/node.py", line 224, in _peer_connected
	    del self.peers[known_peer.name]
	KeyError: 'nl.tgho.priv.sp-prod-adg02.worker.32226.0.4'
	WARNING:bndl.net.watchdog:<Peer:
	

	
support more syntactic sugar for creating accumulators, e.g.:
	acc = ctx.accumulator(set(), 'add')
	def task(...):
		nonlocal acc
		acc.add(1)
		
		
		
use local read if ctx.files on same node


Support shuffle with not all workers / use at runtime task dependencies



Implement cassandra.limit as
	docs = ctx.cassandra_table('adg', 'document', contact_points='sp-prod-adg01')

	In [27]: 10000 / sum(len(p.token_ranges) for p in docs.parts())
	Out[27]: 7.027406886858749
	
	In [28]: docs.limit(7).count(push_down=False)
	Out[28]: 9957

