Communicate worker list in scheduling task
 - (pin a job on workers or check that shuffles are ok after workers join)

Detect missing data on shuffle read (building on #1) and fail task
 - but with rescheduling the dependency

reschedule failed tasks on other hosts
 - heartbeat at task level to identify failed tasks

don't propagate CTRL+C to workers in ipython shell

dset.sort().first() is waaaay slower than dset.sort().count()

sort should be lazy


check on cancelling jobs
 - use https://docs.python.org/2/reference/expressions.html#generator.close ?

check on partial iteration with e.g. itake and friends

check stable ids for nodes
 - it causes a new node on an old address to be rejected
 - perhaps drop node id and use hostname and port?
   then use each address as key in peertable (node.peers)
   might just as well have issues with multiple hostnames resolving to the same machine ... 

the rx tx rates aren't updated on sp-dev
only start gui on driver by default

investigate cassandra retry policy

pickle ops in the map / filter partitions
 - this may cloudpickle functions so that this 'requirement' doesn't ripple out into the entire dset dag
same for filenames, nparrays, etc.


Limit resource usage from supervisor with: https://docs.python.org/3.4/library/resource.html
Enhance dashboard with info from https://pypi.python.org/pypi/psutil


Show cancellation of tasks / stages / jobs

Don't skip job ids for branches
(assign them on execute?)


wait for gossip to settle before starting a node / the shell
 - e.g. with ctx.worker_count
   and/or sum(t.result() for t in [w.run_task(lambda w: sum(1 for w in w.peers.values() if w.is_connected)) for w in ctx.workers])


move caches, broadcast values etc down to execute layer
use hierarchical namespace for set / del


Use custom pickler to set node in Execute/ComputeContext
 - allow ...Context to be more easily used in e.g. a mapper
 - ex. ctx.range(20).map(lambda i: ctx.node...)
 
 
Possibly introduce a control and data layer
 - i.e. to allow sending large volumes out of band from the control comms
 - this allows the watchdog to operate better as well
 - should improve stability
 
 

Consider not using 1 fixed worker per core, but 1 worker per node which forks per task
 - implement in execute layer
 - Use custom pickler to fetch broadcast value on receive (to ensure the data is available before forking)
 - a lazy alternative to the above would still require the broadcasted data to be available per executing process
 - caching could work through caching the data in pickled/marshalled form
   - although not having to unpickle when using cache was one of the benefits of not using pyspark
 - an advantage would be automatic cleanup after jobs have ran (less leakage)
 ! a major hurdle would be communication with e.g. Cassandra,
   - can't be inherited by a forking process and still work :(
   - would require performing the IO in the main process, and processing in a forked worker
 ! from the python docs:
   - Note that safely forking a multithreaded process is problematic.
 IDEA DROPPED:
   - the two points above are prohibitive
   - figure out an alternative memory efficient broadcast   
 
 
Consider a communication architecture with a core cross node network and a on node inter process network
 - perhaps supplemented by (temporary) direct connections on a data layer
 
 
Make entire compute.dataset api asynchronous
 - i.e. some_action() returns a future
 - use some_action().result() to get the results (may be an iterable)
 - use some_action().cancel() to cancel the job
 

Allow stages to be composed of an undetermined amount of tasks
 - i.e. allow a generator / queue of tasks
 
Allow jobs to be composed of an undetermined amount of stages
 - or put differently, give jobs the ability to yield barriers
 
The API / data model might change into a job which yields 1+ tasks and 0+ barriers.

Consider changing the task execution model from push to pull. 
 - could easy implementing having a task in flight
 - might be difficult with a generator / queue of tasks in combination with worker preferences

 
Spill shuffles to disk
 - and do the sorting in place
 
 
Implement checkpointing
 - perform cleanup of stages before the stage of the checkpointed dset 
 
 
 
Be more carefull in base.py with [task.result() for task in tasks]
 - try except around each task.result()
 
 
add pickle options to broadcast_pickle
 
 
decouple yielding results from job/stage/task executions from driving their execution
  - yield returns when the receiver is done ... all that time, no tasks are scheduled
  - may check the hypotheses above ...
 
 
Prevent occurences of:
 27158 : OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 4127376 max
27158 : OpenBLAS blas_thread_init: pthread_create: Resource temporarily unavailable
 